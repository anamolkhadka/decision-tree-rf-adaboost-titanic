{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Class (Base Model)\n",
    "- This class implements Decision tree with a fit and predict method.\n",
    "- It accepts following input parameters when intializing the object: criterion, max_depth, min_samples_split, min_samples_leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(self, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1):\n",
    "        self.criterion = criterion  # 'gini', 'entropy', or 'misclassification'\n",
    "        self.max_depth = max_depth if max_depth is not None else 100  # Infinite depth if not specified\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.tree = None  # Root of the tree\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # \"\"\" Train the decision tree \"\"\"\n",
    "        # self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "        \"\"\" Train the decision tree with optional sample weighting \"\"\"\n",
    "        if sample_weight is not None:\n",
    "            # Normalize sample weights so they sum to 1\n",
    "            sample_weight = sample_weight / np.sum(sample_weight)\n",
    "\n",
    "            # Resample data based on sample weights\n",
    "            indices = np.random.choice(len(y), size=len(y), replace=True, p=sample_weight)\n",
    "            X = X[indices]\n",
    "            y = y[indices]\n",
    "\n",
    "        self.tree = self._build_tree(X, y, depth=0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for input samples \"\"\"\n",
    "        return np.array([self._traverse_tree(sample, self.tree) for sample in X])\n",
    "    \n",
    "    def _build_tree(self, X, y, depth):\n",
    "        \"\"\" Recursively build the decision tree \"\"\"\n",
    "        num_samples, num_features = X.shape\n",
    "        \n",
    "        # Stopping conditions\n",
    "        if depth >= self.max_depth or num_samples < self.min_samples_split or len(set(y)) == 1:\n",
    "            return self._create_leaf(y)\n",
    "        \n",
    "        # Find the best split\n",
    "        best_feature, best_threshold = self._find_best_split(X, y)\n",
    "        if best_feature is None:\n",
    "            return self._create_leaf(y)\n",
    "        \n",
    "        # Split the dataset\n",
    "        left_mask = X[:, best_feature] <= best_threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        left_subtree = self._build_tree(X[left_mask], y[left_mask], depth + 1)\n",
    "        right_subtree = self._build_tree(X[right_mask], y[right_mask], depth + 1)\n",
    "        \n",
    "        return {'feature': best_feature, 'threshold': best_threshold, 'left': left_subtree, 'right': right_subtree}\n",
    "    \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\" Find the best feature and threshold to split on \"\"\"\n",
    "        best_gain = -1\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        for feature in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature])\n",
    "            for threshold in thresholds:\n",
    "                left_mask = X[:, feature] <= threshold\n",
    "                right_mask = ~left_mask\n",
    "\n",
    "                # Skip if either side is empty\n",
    "                if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = self._calculate_information_gain(X[:, feature], y, threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "\n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _calculate_information_gain(self, feature_column, y, threshold):\n",
    "        \"\"\" Compute the information gain for a given feature split \"\"\"\n",
    "        left_mask = feature_column <= threshold\n",
    "        right_mask = ~left_mask\n",
    "        \n",
    "        if sum(left_mask) < self.min_samples_leaf or sum(right_mask) < self.min_samples_leaf:\n",
    "            return 0\n",
    "        \n",
    "        parent_impurity = self._calculate_impurity(y)\n",
    "        left_impurity = self._calculate_impurity(y[left_mask])\n",
    "        right_impurity = self._calculate_impurity(y[right_mask])\n",
    "        \n",
    "        weight_left = sum(left_mask) / len(y)\n",
    "        weight_right = sum(right_mask) / len(y)\n",
    "        \n",
    "        return parent_impurity - (weight_left * left_impurity + weight_right * right_impurity)\n",
    "    \n",
    "    def _calculate_impurity(self, y):\n",
    "        \"\"\" Compute impurity based on selected criterion \"\"\"\n",
    "        classes, counts = np.unique(y, return_counts=True)\n",
    "        probs = counts / counts.sum()\n",
    "        \n",
    "        if self.criterion == 'gini':\n",
    "            return 1 - np.sum(probs ** 2)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return -np.sum(probs * np.log2(probs))\n",
    "        elif self.criterion == 'misclassification':\n",
    "            return 1 - np.max(probs)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion: Choose 'gini', 'entropy', or 'misclassification'\")\n",
    "    \n",
    "    def _create_leaf(self, y):\n",
    "        \"\"\" Create a leaf node with majority class label \"\"\"\n",
    "        if len(y) == 0:\n",
    "            return {'label': 1}  # Default to class '1' to prevent errors\n",
    "    \n",
    "        values, counts = np.unique(y, return_counts=True)\n",
    "        return {'label': values[np.argmax(counts)]}\n",
    "    \n",
    "    def _traverse_tree(self, sample, node):\n",
    "        \"\"\" Recursively traverse the tree to predict class \"\"\"\n",
    "        if 'label' in node:\n",
    "            return node['label']\n",
    "        if sample[node['feature']] <= node['threshold']:\n",
    "            return self._traverse_tree(sample, node['left'])\n",
    "        else:\n",
    "            return self._traverse_tree(sample, node['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Class (Ensemble Model 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, base_classifier, num_trees=10, min_features=2):\n",
    "        \"\"\"\n",
    "        Initialize the Random Forest Classifier.\n",
    "        :param base_classifier: The base classifier (DecisionTree) used in each tree.\n",
    "        :param num_trees: Number of decision trees in the forest.\n",
    "        :param min_features: Minimum number of features to consider for splitting.\n",
    "        \"\"\"\n",
    "        self.num_trees = num_trees\n",
    "        self.min_features = min_features\n",
    "        self.trees = []\n",
    "        self.feature_indices = []\n",
    "        self.base_classifier = base_classifier\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Random Forest on dataset X and labels y.\n",
    "        :param X: Training data (n_samples, n_features)\n",
    "        :param y: Labels (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        self.trees = []\n",
    "        self.feature_indices = []\n",
    "        \n",
    "        for _ in range(self.num_trees):\n",
    "            # Bootstrap Sampling: Randomly sample n instances with replacement\n",
    "            sample_indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, y_sample = X[sample_indices], y[sample_indices]\n",
    "            \n",
    "            # Feature Selection: Choose random subset of features\n",
    "            num_selected_features = np.random.randint(self.min_features, n_features + 1)\n",
    "            feature_idx = np.random.choice(n_features, num_selected_features, replace=False)\n",
    "            self.feature_indices.append(feature_idx)\n",
    "            \n",
    "            # Train decision tree on selected subset of data and features\n",
    "            tree = self.base_classifier()\n",
    "            tree.fit(X_sample[:, feature_idx], y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class labels for given samples.\n",
    "        :param X: Test data (n_samples, n_features)\n",
    "        :return: Predicted labels (n_samples,)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        \n",
    "        for tree, feature_idx in zip(self.trees, self.feature_indices):\n",
    "            pred = tree.predict(X[:, feature_idx])\n",
    "            predictions.append(pred)\n",
    "        \n",
    "        # Majority voting\n",
    "        final_predictions = np.array([Counter(col).most_common(1)[0][0] for col in np.array(predictions).T])\n",
    "        return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting using AdaBoost Class (Ensemble Model 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, weak_learner, num_learners=50, learning_rate=1.0):\n",
    "        \"\"\"\n",
    "        Initialize AdaBoost classifier.\n",
    "\n",
    "        :param weak_learner: Weak learner model (e.g., DecisionTree)\n",
    "        :param num_learners: Number of weak learners to train\n",
    "        :param learning_rate: Weight applied to weak learners\n",
    "        \"\"\"\n",
    "        self.weak_learner = weak_learner\n",
    "        self.num_learners = num_learners\n",
    "        self.learning_rate = learning_rate\n",
    "        self.learners = [] # List of trained weak learners\n",
    "        self.alphas = [] # List of learner weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier.\n",
    "\n",
    "        :param X: Training data (n_samples, n_features)\n",
    "        :param y: Training labels (-1 or +1) (n_samples,)\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "        # Step 1: Initialize weights equally\n",
    "        weights = np.full(n_samples, (1 / n_samples))\n",
    "\n",
    "        for m in range(self.num_learners):\n",
    "            # Step 2: Train a weak learner on weighted data\n",
    "            learner = self.weak_learner()  # Create a new weak learner\n",
    "            learner.fit(X, y, sample_weight=weights)  # Fit with sample weights\n",
    "\n",
    "            # Step 3: Get predictions\n",
    "            predictions = learner.predict(X)\n",
    "            incorrect = predictions != y  # Boolean array for misclassifications\n",
    "\n",
    "            # Step 4: Compute weighted error\n",
    "            err_m = np.dot(weights, incorrect) / np.sum(weights)\n",
    "\n",
    "            # Step 5: Compute alpha (learner's contribution)\n",
    "            alpha_m = self.learning_rate * np.log((1 - err_m) / (err_m + 1e-10)) / 2\n",
    "\n",
    "            # Step 6: Update sample weights\n",
    "            weights *= np.exp(alpha_m * incorrect * 2)  # Increase weights for misclassified samples\n",
    "            weights /= np.sum(weights)  # Normalize\n",
    "\n",
    "            # Store learner and its alpha\n",
    "            self.learners.append(learner)\n",
    "            self.alphas.append(alpha_m)\n",
    "\n",
    "            # Early stopping if perfect classification is achieved\n",
    "            if err_m == 0:\n",
    "                break\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels for input data.\n",
    "\n",
    "        :param X: Test data (n_samples, n_features)\n",
    "        :return: Predicted class labels (n_samples,)\n",
    "        \"\"\"\n",
    "        # Aggregate predictions from weak learners\n",
    "        final_prediction = np.zeros(X.shape[0])\n",
    "\n",
    "        for alpha, learner in zip(self.alphas, self.learners):\n",
    "            final_prediction += alpha * learner.predict(X)\n",
    "\n",
    "        return np.sign(final_prediction)  # Convert to -1 or +1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with the Titanic Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "- The Titanic dataset is processed in the same way from the reference of the notebook on Kaggle \"Introduction to Decision Trees (Titanic dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Imports needed for the script\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 2: Load the dataset\n",
    "train = pd.read_csv('./data/train.csv')\n",
    "test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# Store our test passenger IDs for easy access\n",
    "PassengerId = test['PassengerId']\n",
    "\n",
    "# Showing overview of the train dataset\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Processed Training Data:\n",
      "   Survived  Pclass  Sex  Age  Parch  Fare  Embarked  Has_Cabin  FamilySize  \\\n",
      "0         0       3    1    1      0     0         0          0           2   \n",
      "1         1       1    0    2      0     3         1          1           2   \n",
      "2         1       3    0    1      0     1         0          0           1   \n",
      "\n",
      "   IsAlone  Title  \n",
      "0        0      1  \n",
      "1        0      3  \n",
      "2        1      4  \n",
      "Training Data Shape: (712, 10)\n",
      "Validation Data Shape: (179, 10)\n",
      "Test Data Shape: (418, 10)\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Feature Engineering\n",
    "\n",
    "# Create a backup copy\n",
    "original_train = train.copy()\n",
    "\n",
    "# Combining train and test dataset\n",
    "full_data = [train, test]\n",
    "\n",
    "# Feature that tells whether a passenger had a cabin on the Titanic\n",
    "# train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "# test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n",
    "\n",
    "train['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "test['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 1 if pd.notna(x) else 0)\n",
    "\n",
    "# Feature engineering steps taken from Sina and Anisotropic\n",
    "# Create new feature FamilySize as a combination of SibSp and Parch\n",
    "for dataset in full_data:\n",
    "    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "# Create new feature IsAlone from FamilySize\n",
    "for dataset in full_data:\n",
    "    dataset['IsAlone'] = 0\n",
    "    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n",
    "\n",
    "# Remove all NULLS in the Embarked column\n",
    "for dataset in full_data:\n",
    "    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n",
    "\n",
    "# Remove all NULLS in the Fare column\n",
    "for dataset in full_data:\n",
    "    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n",
    "\n",
    "# Remove all NULLS in the Age column\n",
    "for dataset in full_data:\n",
    "    age_avg = dataset['Age'].mean()\n",
    "    age_std = dataset['Age'].std()\n",
    "    age_null_count = dataset['Age'].isnull().sum()\n",
    "    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n",
    "    # Next line has been improved to avoid warning\n",
    "    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n",
    "    dataset['Age'] = dataset['Age'].astype(int)\n",
    "\n",
    "# Step 4: Extract Titles from Name\n",
    "\n",
    "# Define function to extract titles from passenger names\n",
    "def get_title(name):\n",
    "    #title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    title_search = re.search(r' ([A-Za-z]+)\\.', name)\n",
    "    # If the title exists, extract and return it.\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Name'].apply(get_title)\n",
    "\n",
    "# Group all non-common titles into one single grouping \"Rare\"\n",
    "for dataset in full_data:\n",
    "    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n",
    "\n",
    "    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n",
    "    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n",
    "\n",
    "# Step 5: Encode Categorical Variables\n",
    "for dataset in full_data:\n",
    "    # Mapping Sex\n",
    "    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n",
    "    \n",
    "    # Mapping titles\n",
    "    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n",
    "    dataset['Title'] = dataset['Title'].map(title_mapping)\n",
    "    dataset['Title'] = dataset['Title'].fillna(0)\n",
    "\n",
    "    # Mapping Embarked\n",
    "    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n",
    "    \n",
    "    # Mapping Fare\n",
    "    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n",
    "    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n",
    "    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare'] = 2\n",
    "    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n",
    "    dataset['Fare'] = dataset['Fare'].astype(int)\n",
    "    \n",
    "    # Mapping Age\n",
    "    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n",
    "    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n",
    "    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n",
    "    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n",
    "    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4\n",
    "\n",
    "# Step 6: Feature Selection: remove variables no longer containing relevant information\n",
    "drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\n",
    "train = train.drop(drop_elements, axis = 1)\n",
    "test  = test.drop(drop_elements, axis = 1)\n",
    "\n",
    "# Step 7: Train-Test Split for Validation\n",
    "X = train.drop(\"Survived\", axis=1)\n",
    "y = train[\"Survived\"]\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Showing processed train data\n",
    "print(\"Final Processed Training Data:\")\n",
    "print(train.head(3))\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Validation Data Shape:\", X_val.shape)\n",
    "print(\"Test Data Shape:\", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "- Evaluating the performance of Decision Tree model, Random forest model, and AdaBoost model with the Titanic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy: 0.8324\n",
      "Random Forest Accuracy: 0.8212\n",
      "AdaBoost Accuracy: 0.6927\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "dt_model = DecisionTree(criterion=\"gini\", max_depth=5, min_samples_split=2, min_samples_leaf=1)\n",
    "rf_model = RandomForest(base_classifier=DecisionTree, num_trees=100, min_features=2)\n",
    "ab_model = AdaBoost(weak_learner=DecisionTree, num_learners=80, learning_rate=0.002)\n",
    "\n",
    "# Train the models\n",
    "dt_model.fit(X_train.values, y_train.values)  # Convert to NumPy for consistency\n",
    "rf_model.fit(X_train.values, y_train.values)\n",
    "ab_model.fit(X_train.values, y_train.values)\n",
    "\n",
    "# Predict on validation set\n",
    "dt_preds = dt_model.predict(X_val.values)\n",
    "rf_preds = rf_model.predict(X_val.values)\n",
    "ab_preds = ab_model.predict(X_val.values)\n",
    "\n",
    "# Compute accuracy\n",
    "dt_acc = accuracy_score(y_val.values, dt_preds)\n",
    "rf_acc = accuracy_score(y_val.values, rf_preds)\n",
    "ab_acc = accuracy_score(y_val.values, ab_preds)\n",
    "\n",
    "# Print results\n",
    "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "print(f\"AdaBoost Accuracy: {ab_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Results in Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance on Titanic Dataset:\n",
      "\n",
      "\n",
      "           Model  Accuracy\n",
      "0  Decision Tree  0.832402\n",
      "1  Random Forest  0.821229\n",
      "2       AdaBoost  0.692737\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    \"Model\": [\"Decision Tree\", \"Random Forest\", \"AdaBoost\"],\n",
    "    \"Accuracy\": [dt_acc, rf_acc, ab_acc]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Performance on Titanic Dataset:\\n\\n\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved: titanic_submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "dt_test_preds = dt_model.predict(test.values)\n",
    "rf_test_preds = rf_model.predict(test.values)\n",
    "ab_test_preds = ab_model.predict(test.values)\n",
    "\n",
    "# Convert to binary output (since the models might return -1, 1). 0 is for not survived, 1 is for survived.\n",
    "dt_test_preds = [1 if pred > 0 else 0 for pred in dt_test_preds]\n",
    "rf_test_preds = [1 if pred > 0 else 0 for pred in rf_test_preds]\n",
    "ab_test_preds = [1 if pred > 0 else 0 for pred in ab_test_preds]\n",
    "\n",
    "# Create a submission DataFrame\n",
    "submission_df = pd.DataFrame({\n",
    "    \"PassengerId\": PassengerId,  # Ensure the ID column is retained\n",
    "    \"DecisionTree\": dt_test_preds,\n",
    "    \"RandomForest\": rf_test_preds,\n",
    "    \"AdaBoost\": ab_test_preds\n",
    "})\n",
    "\n",
    "# Save results as CSV file\n",
    "submission_df.to_csv(\"titanic_predictions.csv\", index=False)\n",
    "print(\"Submission file saved: titanic_submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSE6363",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
